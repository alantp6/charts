# Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.

# Default values for clara.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
images:
  namespace: clara
  controller: controller
  platformapiserver: platformapiserver
  pullSecrets: []
  tag: latest

controller:
  name: clara
  install: false
  serviceAccount: clara-service-account

platformapiserver:
  name: platformapiserver
  serviceAccount: platformapiserver-service-account
  serviceType: NodePort # Alternatively: ClusterIp if only in cluster clients will exist

  # Configuration for the payload service in the Clara Deploy SDK server.
  payloadService:

    # The path on the node running Clara Deploy SDK where payloads will be stored.
    # Every payload that is created by Clara Deploy SDK will exist as a directory inside
    # this path (e.g. "/clara/payloads/c780b2cb-d26b-4151-8e7a-b4fbfb876f69").
    hostVolumePath: "/clara/payloads"

    # The disk capacity to reserve for the payload volume.
    capacity: 10Gi

  # Configuration for the common volume that is mounted (read-only) to all pipeline
  # service containers.
  #
  # The path that is used to mount this volume in each of the deployed pipeline
  # services is provided to those service containers via the
  # NVIDIA_CLARA_SERVICE_DATA_PATH environment variable. For example, a TensorRT
  # Inference Service may be defined in the pipeline using the following command:
  #
  #   --models=$NVIDIA_CLARA_SERVICE_DATA_PATH/models
  #
  # to specify that the models repository used by the service will be stored
  # inside the 'models' subdirectory of this common volume. In the case of the
  # default hostVolumePath of "/clara/common", the path provided for the models
  # repository of the TRTIS container would then correspond to the host path of
  # "/clara/common/models".
  #
  # Since this volume is mounted read-only by services, it is expected that the
  # path to this volume already exists on the host and is populated with any
  # required data for the services that use it (such as models used by a TRTIS
  # service). If it does not exist at deployment time, it will be created and
  # will be empty.
  commonServiceVolume:

    # The path on the node running Clara Deploy SDK where the common volume will
    # be stored.
    hostVolumePath: "/clara/common"

    # The disk capacity to reserve for the common volume.
    capacity: 10Gi

  # Configuration for the service volume used by the Clara Deploy SDK server.
  #
  # Each service that is deployed by Clara Deploy SDK using a 'volume' connection
  # is provided with a persistent volume to which each of the volume connections
  # are mounted. For example, this volume connection:
  #
  #   volume:
  #   - name: VOlUME_PATH
  #     path: /var/www
  #
  # Will provide a volume mount inside the service container that corresponds to
  # the host path of (using the default hostVolumePath)
  #
  #   /clara/service-volumes/{serviceId}/var/www
  #
  # where {serviceId} is the unique ID generated by Clara Deploy SDK that was used
  # to deploy the service.
  serviceVolume:

    # The path on the node running Clara Deploy SDK where service volumes will be stored.
    # Every service that is deployed by Clara Deploy SDK will have a volume mounted in this
    # path (e.g. "/clara/service-volumes/c780b2cb-d26b-4151-8e7a-b4fbfb876f69").
    hostVolumePath: "/clara/service-volumes"

    # The disk capacity to reserve for the service volume.
    capacity: 10Gi

######################################################
#      Configuration Values for Argo Dependency     #
######################################################
argo-for-clara:
  images:
    namespace: argoproj
    tag: v2.2.1
  minio:
    install: false
    defaultBucket:
      enabled: false

######################################################
#      Configuration Values for Grafana Dependency     #
######################################################
grafana:
  image:
    repository: grafana/grafana
    tag: 6.3.3
    pullPolicy: IfNotPresent
  service:
    type: NodePort
    nodePort: 32000
  adminPassword: clara123
  grafana.ini:
    users:
      login_hint: admin
      password_hint: clara123
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - name: Elasticsearch
        type: elasticsearch
        tyleLogoUrl: "public/app/plugins/datasource/elasticsearch/img/elasticsearch.svg"
        access: proxy
        url: "http://elasticsearch-master:9200"
        database: "log*"
        isDefault: true
        jsonData:
          esVersion: 70
          maxConcurrentShardRequests: 256
          timeField: "@timestamp"
          logMessageField: "message"
          logLevelField: "stream"


##############################################################
#      Configuration Values for Elasticsearch Dependency     #
##############################################################
elasticsearch:
  antiAffinity: "soft"
  replicas: 1
  resources:
    # need more cpu upon initialization, therefore burstable class
    limits:
      cpu: 1000m
    requests:
      cpu: 100m
  persistentVolumeSize: 10Gi
  # When persistence is enbabled the elasticsearch helm chart
  # provisions persistent volume claims. Helm does not manage
  # provisioned persistent volume claims generated in StatefulSets,
  # so helm does not delete them. Consequently, when persistence 
  # is enabled and we uninstall clara the persistent volume claim
  # does not get cleaned up. This causes a new deployment to fail
  # because the persistent volume claim already exists.
  # To address this issue, we disable persistence and mount an
  # "extra volume" that uses a volume claim that is managed by helm.
  #
  # Here are a few important links associated with this issue:
  # - Dynamic volume provisioning: https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/
  # - Issue related to helm not managing pvc: https://github.com/helm/helm/issues/3313
  persistence:
    enabled: false
  extraVolumes: |
    - name: {{ template "uname" . }}
      mountPath: /usr/share/elasticsearch/data
      persistentVolumeClaim:
        claimName: {{ .Release.Name }}-elasticsearch-volume-claim
  extraInitContainers: |
    - name: fix-permissions
      image: busybox
      command: ["sh", "-c", "chown -R 1000:1000 /usr/share/elasticsearch/data"]
      securityContext:
        privileged: true
      volumeMounts:
      - name: {{ template "uname" . }}
        mountPath: /usr/share/elasticsearch/data
    - name: increase-vm-max-map
      image: busybox
      command: ["sysctl", "-w", "vm.max_map_count=262144"]
      securityContext:
        privileged: true
    - name: increase-fd-ulimit
      image: busybox
      command: ["sh", "-c", "ulimit -n 65536"]
      securityContext:
        privileged: true

######################################################################
#      Configuration Values for Fluentd-Elasticsearch Dependency     #
######################################################################
fluentd-elasticsearch:
  elasticsearch:
    host: "elasticsearch-master"
