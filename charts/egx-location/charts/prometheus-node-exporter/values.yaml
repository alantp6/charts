# Default values for prometheus-node-exporter.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 1
maxUnavailable: "10%"

image:
  pullSecrets:
    - "quayio-userpass"

# node exporter container
container:
  image:
    repository: quay.io/prometheus/node-exporter
    tag: v0.18.0
    pullPolicy: IfNotPresent

  args:
  - --path.rootfs=/host
  - --collector.textfile.directory=/run/prometheus

  service:
    type: ClusterIP
    port: 9100
    targetPort: 9100

  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      cpu: 200m
      memory: 512Mi

  volumeMounts:
  - name: rootfs
    mountPath: /host
    readOnly:  true
  - name: collector-textfiles
    readOnly: true
    mountPath: /run/prometheus

service:
  type: ClusterIP
  port: 9100
  targetPort: 9100
  nodePort:
  annotations:
    prometheus.io/scrape: "true"

# dcgm exporter container
containerDCGM:
  image:
    repository: nvidia/dcgm-exporter
    tag: 1.4.6
    pullPolicy: Always

  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 512Mi

  volumeMounts:
  - name: device-metrics
    mountPath: /run/prometheus

# dcgm container with dcp
container3DCP:
  image:
    repository: quay.io/nvidia/dcgm-exporter-dcp
    tag: 1.6.3
    pullPolicy: Always

  volumeMounts:
  - name: device-metrics
    mountPath: /run/prometheus

# dcgm container with dcp v 1.6.6
container3DCP166:
  image:
    repository: quay.io/nvidia/dcgm-exporter-dcp
    tag: 1.6.6
    pullPolicy: Always

  volumeMounts:
  - name: device-metrics
    mountPath: /run/prometheus

# dcgm pod details exporter
dcgmPodExporterContainer:
  image:
    repository: nvidia/pod-devices-exporter
    tag: 1.4.6-dcgm
    pullPolicy: Always

  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      cpu: 200m
      memory: 512Mi

  volumeMounts:
  - name: pod-device-resources
    readOnly: true
    mountPath: /var/lib/kubelet/pod-resources
  - name: device-metrics
    readOnly: true
    mountPath: /run/prometheus
  - name: collector-textfiles
    mountPath: /run/dcgm

# fscache metrics exporter
fcacheExporterContainer:
  enabled: true
  image:
    repository: quay.io/nvidia/fscache-exporter
    tag: 0.1
    pullPolicy: Always

  volumeMounts:
  - name: collector-textfiles
    mountPath: /run/prometheus

prometheus:
  monitor:
    enabled: false
    additionalLabels: {}
    namespace: ""

    scrapeTimeout: 10s

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 200m
  #   memory: 50Mi
  # requests:
  #   cpu: 100m
  #   memory: 30Mi

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:
  imagePullSecrets: []

securityContext:
  runAsNonRoot: true
  runAsUser: 65534

rbac:
  ## If true, create & use RBAC resources
  ##
  create: true
  ## If true, create & use Pod Security Policy resources
  ## https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  pspEnabled: true

# for deployments that have node_exporter deployed outside of the cluster, list
# their addresses here
endpoints: []

# Expose the service to the host network
hostNetwork: true

## Assign a group of affinity scheduling rules
##
# affinity: {}
#   nodeAffinity:
#     requiredDuringSchedulingIgnoredDuringExecution:
#       nodeSelectorTerms:
#         - matchFields:
#             - key: metadata.name
#               operator: In
#               values:
#                 - target-host-name
## node affinity for where to deploy this daemonset
affinity: {}
affinityGPU: {}

## Assign a nodeSelector if operating a hybrid cluster
##
nodeSelector: {}
#   beta.kubernetes.io/arch: amd64
#   beta.kubernetes.io/os: linux

tolerations:
  - effect: NoSchedule
    operator: Exists

volumes:
- name: collector-textfiles
  emptyDir:
    medium: Memory
- name: device-metrics
  emptyDir:
    medium: Memory
- name: pod-device-resources
  hostPath:
    path: /var/lib/kubelet/pod-resources
- name: rootfs
  hostPath:
    path: /rootfs

## Assign a PriorityClassName to pods if set
# priorityClassName: ""

## Additional container arguments
##
extraArgs: {}
#   - --collector.diskstats.ignored-devices=^(ram|loop|fd|(h|s|v)d[a-z]|nvme\\d+n\\d+p)\\d+$

## Additional mounts from the host
##
extraHostVolumeMounts: {}
#  - name: <mountName>
#    hostPath: <hostPath>
#    mountPath: <mountPath>
#    readOnly: true|false
#    mountPropagation: None|HostToContainer|Bidirectional

## Additional configmaps to be mounted.
##
configmaps: {}
# - name: <configMapName>
#   mountPath: <mountPath>

# EGX settings:
locationName: ""
pushProxyClient:
  enabled: false
  image:
    repository: quay.io/ligenvidia/pushprox-client
    tag: "1.0"
    pullPolicy: Always
  proxyUrl: ""
